{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7mFhPkKlz5I"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import pandas as pd  # For optional Pandas DataFrame handling\n",
        "\n",
        "def scrape_website(url, data_type):\n",
        "    \"\"\"\n",
        "    Scrapes a website for specified data and saves it to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the website to scrape.\n",
        "        data_type: The type of data to scrape (e.g., \"headlines\", \"products\", \"jobs\").\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        all_data = []\n",
        "\n",
        "        if data_type == \"headlines\":\n",
        "            headlines = scrape_headlines(soup)\n",
        "            all_data.extend(headlines) # handles pagination if available\n",
        "\n",
        "        elif data_type == \"products\":\n",
        "            products = scrape_products(soup)\n",
        "            all_data.extend(products)\n",
        "            # Add pagination handling if needed for product pages\n",
        "\n",
        "        elif data_type == \"jobs\":\n",
        "            jobs = scrape_jobs(soup)\n",
        "            all_data.extend(jobs)\n",
        "            # Add pagination handling if needed for job listings\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid data type specified.\")\n",
        "            return\n",
        "\n",
        "        if all_data: # Check if any data was extracted\n",
        "            save_to_csv(all_data, f\"{data_type}_data.csv\")\n",
        "            # Or use pandas for more structured data handling:\n",
        "            # df = pd.DataFrame(all_data)\n",
        "            # df.to_csv(f\"{data_type}_data.csv\", index=False)  # Save to CSV using pandas\n",
        "            print(f\"Data scraped and saved to {data_type}_data.csv\")\n",
        "        else:\n",
        "            print(\"No data found matching the specified criteria.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "def scrape_headlines(soup):\n",
        "    headlines = []\n",
        "    for h2_tag in soup.find_all(\"h2\"):  # Example: Scrape h2 tags for headlines\n",
        "        headline_text = h2_tag.text.strip()\n",
        "        if headline_text: # Check for empty strings\n",
        "          headlines.append({\"headline\": headline_text})  # Store in dictionary for CSV\n",
        "    return headlines\n",
        "\n",
        "\n",
        "def scrape_products(soup):\n",
        "    products = []\n",
        "    # Implement your logic to extract product details (name, price, etc.)\n",
        "    # Example (replace with your website's specific structure):\n",
        "    for product_item in soup.find_all(\"div\", class_=\"product-item\"):  # Example class\n",
        "        name = product_item.find(\"a\", class_=\"product-name\").text.strip() if product_item.find(\"a\", class_=\"product-name\") else \"N/A\"\n",
        "        price = product_item.find(\"span\", class_=\"product-price\").text.strip() if product_item.find(\"span\", class_=\"product-price\") else \"N/A\"\n",
        "        products.append({\"name\": name, \"price\": price})\n",
        "    return products\n",
        "\n",
        "\n",
        "def scrape_jobs(soup):\n",
        "    jobs = []\n",
        "    # Implement your logic to extract job details (title, company, location, etc.)\n",
        "    # Example (replace with your website's specific structure):\n",
        "    for job_listing in soup.find_all(\"li\", class_=\"job-listing\"):  # Example class\n",
        "        title = job_listing.find(\"h3\").text.strip() if job_listing.find(\"h3\") else \"N/A\"\n",
        "        company = job_listing.find(\"span\", class_=\"company\").text.strip() if job_listing.find(\"span\", class_=\"company\") else \"N/A\"\n",
        "        jobs.append({\"title\": title, \"company\": company})\n",
        "    return jobs\n",
        "\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    if data:\n",
        "        keys = data[0].keys()  # Get the keys (column names) from the first dictionary\n",
        "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(data)\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "url = input(\"Enter the URL of the website: \")\n",
        "data_type = input(\"Enter the type of data to scrape (headlines, products, jobs): \").lower()\n",
        "\n",
        "scrape_website(url, data_type)"
      ]
    }
  ]
}